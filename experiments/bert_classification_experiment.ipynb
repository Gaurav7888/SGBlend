{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models, constraints, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# ======================\n",
    "# Custom Constraints (Same as Before)\n",
    "# ======================\n",
    "class ClipConstraint(constraints.Constraint):\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return tf.clip_by_value(w, self.min_val, self.max_val)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'min_val': self.min_val, 'max_val': self.max_val}\n",
    "\n",
    "# ======================\n",
    "# Custom Activation Layers (Same as Before)\n",
    "# ======================\n",
    "class SGBlend(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Learnable parameters\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\",\n",
    "            shape=(1,),\n",
    "            initializer=\"zeros\",  # Start with GELU (alpha=0)\n",
    "            constraint=tf.keras.constraints.MinMaxNorm(0.0, 1.0)\n",
    "        )\n",
    "\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",\n",
    "            shape=(1,),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            constraint=ClipConstraint(0.1, 10.0)\n",
    "        )\n",
    "\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\",\n",
    "            shape=(1,),\n",
    "            initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # GELU approximation: x * σ(1.702x)\n",
    "        gelu_term = x * tf.nn.sigmoid(1.702 * x)\n",
    "        # SSwish: x * σ(beta x) - gamma\n",
    "        sswish_term = x * tf.nn.sigmoid(self.beta * x) - self.gamma\n",
    "        # Blend using alpha (sigmoid to enforce [0,1])\n",
    "        alpha = tf.nn.sigmoid(self.alpha)  # α ∈ [0,1]\n",
    "        return alpha * sswish_term + (1 - alpha) * gelu_term\n",
    "\n",
    "class SSwish(layers.Layer):\n",
    "    \"\"\"Symmetric Swish\"\"\"\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=(1,),\n",
    "            initializer='glorot_uniform',\n",
    "            constraint=ClipConstraint(0.1, 10)\n",
    "        )\n",
    "        self.gamma = self.add_weight(\n",
    "            name='gamma',\n",
    "            shape=(1,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * tf.nn.sigmoid(self.beta * x) - self.gamma\n",
    "\n",
    "class GELU(layers.Layer):\n",
    "    \"\"\"Gaussian Error Linear Unit\"\"\"\n",
    "    def call(self, x):\n",
    "        return tf.nn.gelu(x)\n",
    "\n",
    "class Mish(layers.Layer):\n",
    "    \"\"\"Mish Activation\"\"\"\n",
    "    def call(self, x):\n",
    "        return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# ======================\n",
    "# Positional Encoding (Same as Before)\n",
    "# ======================\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "    angle_rates = 1 / (10000**depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_length=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(max_length, d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "# ======================\n",
    "# BERT Components\n",
    "# ======================\n",
    "def encoder_block(inputs, embed_dim, num_heads, ff_dim, activation, rate=0.1):\n",
    "    \"\"\"Transformer encoder block with configurable activation.\"\"\"\n",
    "    # Self-Attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn_output = layers.Dropout(rate)(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    if isinstance(activation, layers.Layer):\n",
    "        ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim),\n",
    "            activation,\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "    else:\n",
    "        ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=activation),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "\n",
    "    ffn_output = ffn(out1)\n",
    "    ffn_output = layers.Dropout(rate)(ffn_output)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# ======================\n",
    "# Load and Preprocess IMDB Dataset\n",
    "# ======================\n",
    "def load_imdb_data(max_samples=25000, max_length=256):\n",
    "    \"\"\"Load and preprocess IMDB movie reviews dataset.\"\"\"\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "    train_data, test_data = dataset['train'], dataset['test']\n",
    "\n",
    "    # Take subsets\n",
    "    train_data = train_data.take(max_samples)\n",
    "    test_data = test_data.take(max_samples//2)\n",
    "\n",
    "    # Process texts and labels\n",
    "    train_texts, train_labels = [], []\n",
    "    test_texts, test_labels = [], []\n",
    "\n",
    "    for text, label in tfds.as_numpy(train_data):\n",
    "        train_texts.append('[CLS] ' + text.decode('utf-8'))\n",
    "        train_labels.append(label)\n",
    "\n",
    "    for text, label in tfds.as_numpy(test_data):\n",
    "        test_texts.append('[CLS] ' + text.decode('utf-8'))\n",
    "        test_labels.append(label)\n",
    "\n",
    "    # Split into train/validation\n",
    "    split = int(len(train_texts) * 0.9)\n",
    "    x_train, x_val = train_texts[:split], train_texts[split:]\n",
    "    y_train, y_val = train_labels[:split], train_labels[split:]\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(oov_token='[UNK]')\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "    # Convert to sequences\n",
    "    x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "    x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
    "    x_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "    # Pad sequences\n",
    "    x_train_pad = pad_sequences(x_train_seq, maxlen=max_length, padding='post')\n",
    "    x_val_pad = pad_sequences(x_val_seq, maxlen=max_length, padding='post')\n",
    "    x_test_pad = pad_sequences(x_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Convert labels to arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "    y_test = np.array(test_labels)\n",
    "\n",
    "    print(f\"Vocabulary size: {len(tokenizer.word_index)+1}\")\n",
    "    print(f\"Training samples: {len(x_train_pad)}\")\n",
    "    print(f\"Validation samples: {len(x_val_pad)}\")\n",
    "\n",
    "    return (x_train_pad, y_train), (x_val_pad, y_val), tokenizer\n",
    "\n",
    "# ======================\n",
    "# BERT Model Architecture\n",
    "# ======================\n",
    "def create_bert_model(activation, vocab_size, max_length=256,\n",
    "                     d_model=64, num_heads=2, ff_dim=128, num_layers=2):\n",
    "    \"\"\"Create a BERT-like model for text classification.\"\"\"\n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    x = PositionalEmbedding(vocab_size, d_model, max_length)(inputs)\n",
    "\n",
    "    # Stack encoder blocks\n",
    "    for _ in range(num_layers):\n",
    "        x = encoder_block(x, d_model, num_heads, ff_dim, activation)\n",
    "\n",
    "    # Use [CLS] token for classification\n",
    "    cls_token = x[:, 0, :]\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(cls_token)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# ======================\n",
    "# Training and Evaluation\n",
    "# ======================\n",
    "def run_experiment():\n",
    "    # Load data\n",
    "    (x_train, y_train), (x_val, y_val), tokenizer = load_imdb_data()\n",
    "\n",
    "    # Define activations to compare\n",
    "    activations = {\n",
    "        'SGBlend': SGBlend(),\n",
    "        'GELU': GELU(),\n",
    "        'Swish': tf.nn.swish,\n",
    "        'SSwish': SSwish(),\n",
    "        'ReLU': tf.nn.relu,\n",
    "        'Mish': Mish(),\n",
    "    }\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 25\n",
    "    batch_size = 32\n",
    "    results = []\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Neuron activity measurement\n",
    "    def measure_neuron_activity(model, x_data, layer_idx=-2):\n",
    "        layer_outputs = [layer.output for layer in model.layers[:layer_idx]]\n",
    "        activations_model = tf.keras.Model(inputs=model.inputs, outputs=layer_outputs)\n",
    "\n",
    "        sample_data = x_data[:100]\n",
    "        activations = activations_model.predict([sample_data])\n",
    "        penultimate = activations[-1]\n",
    "\n",
    "        if len(penultimate.shape) > 2:\n",
    "            dead = np.mean(np.max(penultimate, axis=(0,1)) < 1e-3)\n",
    "        else:\n",
    "            dead = np.mean(np.max(penultimate, axis=0) < 1e-3)\n",
    "\n",
    "        return np.mean(dead)\n",
    "\n",
    "    # Training loop\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            for name, activation in activations.items():\n",
    "                print(f\"\\nTraining with {name} activation...\")\n",
    "\n",
    "                model = create_bert_model(\n",
    "                    activation,\n",
    "                    len(tokenizer.word_index)+1,\n",
    "                    max_length=256\n",
    "                )\n",
    "\n",
    "                model.compile(\n",
    "                    optimizer=optimizers.Adam(1e-4),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "\n",
    "                start = time.time()\n",
    "                history = model.fit(\n",
    "                    x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1\n",
    "                )\n",
    "                train_time = time.time() - start\n",
    "\n",
    "                # Evaluation\n",
    "                val_loss, val_acc = model.evaluate(x_val, y_val, verbose=0)\n",
    "                dead_neurons = measure_neuron_activity(model, x_train)\n",
    "\n",
    "                results.append({\n",
    "                    'name': name,\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'training_time': train_time,\n",
    "                    'dead_neurons': dead_neurons,\n",
    "                    'history': history.history\n",
    "                })\n",
    "\n",
    "                print(f\"{name} | Val Acc: {val_acc:.4f} | Time: {train_time:.1f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"GPU Error: {e}, falling back to CPU...\")\n",
    "        # CPU training loop (same as above)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "# Visualization and Reporting (Same Structure)\n",
    "# ======================\n",
    "def visualize_results(results):\n",
    "    activations = {\n",
    "        'AdaptiveSSwishGELU': AdaptiveSSwishGELU(),\n",
    "        'GELU_SSwish': GELU_SSwish(),\n",
    "        'GELU': GELU(),\n",
    "        'Swish': tf.nn.swish,\n",
    "        'SSwish': SSwish(),\n",
    "        'ReLU': tf.nn.relu,\n",
    "        'Mish': Mish(),\n",
    "    }\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # Validation Accuracy\n",
    "    plt.subplot(2,2,1)\n",
    "    for res in results:\n",
    "        plt.plot(res['history']['val_accuracy'], label=res['name'])\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Validation Loss\n",
    "    plt.subplot(2,2,2)\n",
    "    for res in results:\n",
    "        plt.plot(res['history']['val_loss'], label=res['name'])\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Training Time\n",
    "    plt.subplot(2,2,3)\n",
    "    times = [res['training_time'] for res in results]\n",
    "    plt.bar(activations.keys(), times)\n",
    "    plt.title('Training Time')\n",
    "\n",
    "    # Dead Neurons\n",
    "    plt.subplot(2,2,4)\n",
    "    dead = [res['dead_neurons'] for res in results]\n",
    "    plt.bar(activations.keys(), dead)\n",
    "    plt.title('Dead Neurons Percentage')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bert_activation_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "def output_comparison_table(results):\n",
    "    df = pd.DataFrame([{\n",
    "        'Activation': r['name'],\n",
    "        'Val Accuracy': f\"{r['val_acc']:.4f}\",\n",
    "        'Val Loss': f\"{r['val_loss']:.4f}\",\n",
    "        'Training Time (s)': f\"{r['training_time']:.1f}\",\n",
    "        'Dead Neurons (%)': f\"{r['dead_neurons']:.2%}\"\n",
    "    } for r in results])\n",
    "\n",
    "    return df.sort_values('Val Accuracy', ascending=False)\n",
    "\n",
    "# ======================\n",
    "# Main Execution\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    results = run_experiment()\n",
    "    visualize_results(results)\n",
    "    df = output_comparison_table(results)\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(df)\n",
    "    df.to_csv('bert_activation_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
