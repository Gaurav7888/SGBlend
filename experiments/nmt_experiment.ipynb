{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models, constraints, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# ======================\n",
    "# Custom Constraints\n",
    "# ======================\n",
    "class ClipConstraint(constraints.Constraint):\n",
    "    \"\"\"Clips weights between `min_val` and `max_val`.\"\"\"\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return tf.clip_by_value(w, self.min_val, self.max_val)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'min_val': self.min_val, 'max_val': self.max_val}\n",
    "\n",
    "# ======================\n",
    "# Custom Activation Layers\n",
    "# ======================\n",
    "class AdaptiveSSwishGELU(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Learnable parameters\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\",\n",
    "            shape=(1,),\n",
    "            initializer=\"zeros\",  # Start with GELU (alpha=0)\n",
    "            constraint=tf.keras.constraints.MinMaxNorm(0.0, 1.0)\n",
    "        )\n",
    "\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",\n",
    "            shape=(1,),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            constraint=ClipConstraint(0.1, 10.0)\n",
    "        )\n",
    "\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\",\n",
    "            shape=(1,),\n",
    "            initializer=\"zeros\"\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # GELU approximation: x * σ(1.702x)\n",
    "        gelu_term = x * tf.nn.sigmoid(1.702 * x)\n",
    "        # SSwish: x * σ(beta x) - gamma\n",
    "        sswish_term = x * tf.nn.sigmoid(self.beta * x) - self.gamma\n",
    "        # Blend using alpha (sigmoid to enforce [0,1])\n",
    "        alpha = tf.nn.sigmoid(self.alpha)  # α ∈ [0,1]\n",
    "        return alpha * sswish_term + (1 - alpha) * gelu_term\n",
    "\n",
    "class SSwish(layers.Layer):\n",
    "    \"\"\"Symmetric Swish\"\"\"\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=(1,),\n",
    "            initializer='glorot_uniform',\n",
    "            constraint=ClipConstraint(0.1, 10)\n",
    "        )\n",
    "        self.gamma = self.add_weight(\n",
    "            name='gamma',\n",
    "            shape=(1,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * tf.nn.sigmoid(self.beta * x) - self.gamma\n",
    "\n",
    "class GELU(layers.Layer):\n",
    "    \"\"\"Gaussian Error Linear Unit\"\"\"\n",
    "    def call(self, x):\n",
    "        return tf.nn.gelu(x)\n",
    "\n",
    "class Mish(layers.Layer):\n",
    "    \"\"\"Mish Activation\"\"\"\n",
    "    def call(self, x):\n",
    "        return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Load and Preprocess WMT 2014 English-German Dataset\n",
    "# ======================\n",
    "def load_wmt_data(max_samples=50000, max_length=50):\n",
    "    \"\"\"Load a subset of the WMT 2014 English-German dataset.\"\"\"\n",
    "    print(\"Loading WMT 2014 English-German dataset...\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset, info = tfds.load('wmt14_translate/de-en', with_info=True, as_supervised=True)\n",
    "    train_dataset = dataset['train']\n",
    "    \n",
    "    # Take a subset for faster training\n",
    "    train_dataset = train_dataset.take(max_samples)\n",
    "    \n",
    "    # Create lists to store data\n",
    "    en_texts = []\n",
    "    de_texts = []\n",
    "    \n",
    "    # Extract texts\n",
    "    for en_text, de_text in tfds.as_numpy(train_dataset):\n",
    "        en_texts.append(en_text.decode('utf-8'))\n",
    "        de_texts.append(de_text.decode('utf-8'))\n",
    "    \n",
    "    # Add start and end tokens to target sequences\n",
    "    de_texts_processed = ['<start> ' + text + ' <end>' for text in de_texts]\n",
    "    \n",
    "    # Create tokenizers\n",
    "    en_tokenizer = Tokenizer(filters='')\n",
    "    en_tokenizer.fit_on_texts(en_texts)\n",
    "    \n",
    "    de_tokenizer = Tokenizer(filters='')\n",
    "    de_tokenizer.fit_on_texts(de_texts_processed)\n",
    "    \n",
    "    # Convert to sequences\n",
    "    en_seqs = en_tokenizer.texts_to_sequences(en_texts)\n",
    "    de_seqs = de_tokenizer.texts_to_sequences(de_texts_processed)\n",
    "    \n",
    "    # Pad sequences\n",
    "    en_pad = pad_sequences(en_seqs, maxlen=max_length, padding='post')\n",
    "    de_pad = pad_sequences(de_seqs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Split into training and validation sets (90/10)\n",
    "    split = int(len(en_pad) * 0.9)\n",
    "    \n",
    "    x_train, x_val = en_pad[:split], en_pad[split:]\n",
    "    y_train, y_val = de_pad[:split], de_pad[split:]\n",
    "    \n",
    "    # Get vocabulary sizes\n",
    "    en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "    de_vocab_size = len(de_tokenizer.word_index) + 1\n",
    "    \n",
    "    print(f\"English vocabulary size: {en_vocab_size}\")\n",
    "    print(f\"German vocabulary size: {de_vocab_size}\")\n",
    "    print(f\"Training samples: {len(x_train)}\")\n",
    "    print(f\"Validation samples: {len(x_val)}\")\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), en_tokenizer, de_tokenizer\n",
    "\n",
    "# ======================\n",
    "# Transformer Components\n",
    "# ======================\n",
    "def positional_encoding(length, depth):\n",
    "    \"\"\"Create positional encoding for transformer.\"\"\"\n",
    "    depth = depth/2\n",
    "    \n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    \n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    \"\"\"Combines embedding with positional encoding.\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_length=50):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(max_length, d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "def encoder_block(inputs, embed_dim, num_heads, ff_dim, activation, rate=0.1):\n",
    "    \"\"\"Transformer encoder block with configurable activation function.\"\"\"\n",
    "    # Multi-head attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim\n",
    "    )(inputs, inputs)\n",
    "    attention_output = layers.Dropout(rate)(attention_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    if isinstance(activation, layers.Layer):\n",
    "        # For custom activation layers (which are layers themselves)\n",
    "        ffn_output = layers.Dense(ff_dim)(out1)\n",
    "        ffn_output = activation(ffn_output)\n",
    "        ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "    else:\n",
    "        # For built-in activation functions\n",
    "        ffn_output = layers.Dense(ff_dim, activation=activation)(out1)\n",
    "        ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "\n",
    "    ffn_output = layers.Dropout(rate)(ffn_output)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "def decoder_block(inputs, context, embed_dim, num_heads, ff_dim, activation, rate=0.1):\n",
    "    \"\"\"Transformer decoder block with configurable activation function.\"\"\"\n",
    "    # Self-attention\n",
    "    self_attention = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim\n",
    "    )(inputs, inputs)\n",
    "    self_attention = layers.Dropout(rate)(self_attention)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + self_attention)\n",
    "    \n",
    "    # Cross-attention\n",
    "    cross_attention = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim\n",
    "    )(out1, context)\n",
    "    cross_attention = layers.Dropout(rate)(cross_attention)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + cross_attention)\n",
    "    \n",
    "    # Feed Forward Network\n",
    "    if isinstance(activation, layers.Layer):\n",
    "        # For custom activation layers (which are layers themselves)\n",
    "        ffn_output = layers.Dense(ff_dim)(out2)\n",
    "        ffn_output = activation(ffn_output)\n",
    "        ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "    else:\n",
    "        # For built-in activation functions\n",
    "        ffn_output = layers.Dense(ff_dim, activation=activation)(out2)\n",
    "        ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "        \n",
    "    ffn_output = layers.Dropout(rate)(ffn_output)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(out2 + ffn_output)\n",
    "\n",
    "# ======================\n",
    "# Define Model Architecture\n",
    "# ======================\n",
    "def create_translation_model(activation, en_vocab_size, de_vocab_size, d_model=64, num_heads=2, ff_dim=128, num_layers=2):\n",
    "    \"\"\"\n",
    "    Create a small transformer model for translation with the specified activation function.\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "    encoder_embedding = PositionalEmbedding(en_vocab_size, d_model)(encoder_inputs)\n",
    "    encoder_outputs = encoder_embedding\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        encoder_outputs = encoder_block(\n",
    "            encoder_outputs, d_model, num_heads, ff_dim, activation\n",
    "        )\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = layers.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "    decoder_embedding = PositionalEmbedding(de_vocab_size, d_model)(decoder_inputs)\n",
    "    decoder_outputs = decoder_embedding\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        decoder_outputs = decoder_block(\n",
    "            decoder_outputs, encoder_outputs, d_model, num_heads, ff_dim, activation\n",
    "        )\n",
    "    \n",
    "    # Final output layer\n",
    "    if isinstance(activation, layers.Layer):\n",
    "        decoder_outputs = layers.Dense(ff_dim)(decoder_outputs)\n",
    "        decoder_outputs = activation(decoder_outputs)\n",
    "    else:\n",
    "        decoder_outputs = layers.Dense(ff_dim, activation=activation)(decoder_outputs)\n",
    "        \n",
    "    outputs = layers.Dense(de_vocab_size, activation=\"softmax\")(decoder_outputs)\n",
    "    \n",
    "    return models.Model(\n",
    "        inputs=[encoder_inputs, decoder_inputs],\n",
    "        outputs=outputs\n",
    "    )\n",
    "\n",
    "# ======================\n",
    "# Training and Evaluation Functions\n",
    "# ======================\n",
    "def prepare_data_for_training(x, y):\n",
    "    \"\"\"\n",
    "    Prepare data for training by creating decoder inputs and targets\n",
    "    \"\"\"\n",
    "    # Decoder inputs (shifted right)\n",
    "    decoder_inputs = y[:, :-1]\n",
    "    # Targets (shifted left)\n",
    "    decoder_targets = y[:, 1:]\n",
    "    \n",
    "    return [x, decoder_inputs], decoder_targets\n",
    "\n",
    "def bleu_score(y_true, y_pred, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for translation quality evaluation\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    \n",
    "    # Convert predictions to word sequences\n",
    "    y_pred_words = []\n",
    "    for pred in y_pred:\n",
    "        # Get most likely token at each position\n",
    "        indices = np.argmax(pred, axis=-1)\n",
    "        # Convert indices to words\n",
    "        words = [tokenizer.index_word.get(i, '') for i in indices if i > 0]\n",
    "        y_pred_words.append(words)\n",
    "    \n",
    "    # Convert true values to word sequences\n",
    "    y_true_words = []\n",
    "    for true in y_true:\n",
    "        # Convert indices to words, skipping padding (0)\n",
    "        words = [[tokenizer.index_word.get(i, '') for i in true if i > 0]]\n",
    "        y_true_words.append(words)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    return corpus_bleu(y_true_words, y_pred_words)\n",
    "\n",
    "# ======================\n",
    "# Main Experiment\n",
    "# ======================\n",
    "def run_experiment():\n",
    "    # Load and preprocess data\n",
    "    (x_train, y_train), (x_val, y_val), en_tokenizer, de_tokenizer = load_wmt_data()\n",
    "    \n",
    "    # Prepare data for training\n",
    "    train_data, train_targets = prepare_data_for_training(x_train, y_train)\n",
    "    val_data, val_targets = prepare_data_for_training(x_val, y_val)\n",
    "    \n",
    "    # Define activations to compare\n",
    "    activations = {\n",
    "        'AdaptiveSSwishGELU': AdaptiveSSwishGELU(),\n",
    "        'GELU': GELU(),\n",
    "        'Swish': tf.nn.swish,\n",
    "        'SSwish': SSwish(),\n",
    "        'ReLU': tf.nn.relu,\n",
    "        'Mish': Mish(),\n",
    "    }\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # For measuring neuron \"deadness\" in transformers\n",
    "    def measure_neuron_activity(model, x_data, layer_idx=-3):\n",
    "        # Create a submodel to extract intermediate activations\n",
    "        layer_outputs = [layer.output for layer in model.layers[:layer_idx]]\n",
    "        # Create an intermediary model to get these activations\n",
    "        activations_model = tf.keras.Model(inputs=model.inputs, outputs=layer_outputs)\n",
    "        \n",
    "        # Get sample input data\n",
    "        sample_encoder_input = x_data[0][:100]\n",
    "        sample_decoder_input = x_data[1][:100]\n",
    "        \n",
    "        # Get activations\n",
    "        activations = activations_model.predict([sample_encoder_input, sample_decoder_input])\n",
    "        penultimate_activations = activations[-1]  # Get the last layer\n",
    "        \n",
    "        # Calculate percentage of dead neurons\n",
    "        if len(penultimate_activations.shape) > 2:\n",
    "            # For tensors with more than 2 dimensions, flatten the spatial dimensions\n",
    "            dead_neuron_pct = np.mean(np.max(penultimate_activations, axis=(0, 1)) < 1e-3)\n",
    "        else:\n",
    "            # For 2D tensors (batch_size, features)\n",
    "            dead_neuron_pct = np.mean(np.max(penultimate_activations, axis=0) < 1e-3)\n",
    "        \n",
    "        return dead_neuron_pct\n",
    "\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):  # Use GPU if available\n",
    "            for name, activation in activations.items():\n",
    "                print(f\"\\nTraining with {name} activation...\")\n",
    "\n",
    "                # Create model\n",
    "                en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "                de_vocab_size = len(de_tokenizer.word_index) + 1\n",
    "                \n",
    "                model = create_translation_model(\n",
    "                    activation,\n",
    "                    en_vocab_size,\n",
    "                    de_vocab_size,\n",
    "                    d_model=64,\n",
    "                    num_heads=2,\n",
    "                    ff_dim=128,\n",
    "                    num_layers=2\n",
    "                )\n",
    "\n",
    "                # Compile model\n",
    "                model.compile(\n",
    "                    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"]\n",
    "                )\n",
    "\n",
    "                # Train model\n",
    "                start_time = time.time()\n",
    "                history = model.fit(\n",
    "                    train_data, train_targets,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(val_data, val_targets),\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                # Evaluate model\n",
    "                val_loss, val_acc = model.evaluate(val_data, val_targets, verbose=0)\n",
    "                \n",
    "                # Generate predictions for a small subset for BLEU score\n",
    "                sample_size = min(100, len(val_data[0]))\n",
    "                predictions = model.predict([val_data[0][:sample_size], val_data[1][:sample_size]])\n",
    "                \n",
    "                # Calculate BLEU score\n",
    "                bleu = bleu_score(val_targets[:sample_size], predictions, de_tokenizer)\n",
    "\n",
    "                # Measure dead neurons\n",
    "                dead_neurons = measure_neuron_activity(model, train_data)\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'name': name,\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'bleu': bleu,\n",
    "                    'training_time': training_time,\n",
    "                    'dead_neurons': dead_neurons,\n",
    "                    'history': history.history\n",
    "                })\n",
    "\n",
    "                print(f\"{name} - Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}, BLEU: {bleu:.4f}, Time: {training_time:.1f}s, Dead Neurons: {dead_neurons:.2%}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error using GPU: {e}\")\n",
    "        print(\"Falling back to CPU...\")\n",
    "        \n",
    "        # Repeat with CPU\n",
    "        for name, activation in activations.items():\n",
    "            print(f\"\\nTraining with {name} activation...\")\n",
    "\n",
    "            # Create model\n",
    "            en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "            de_vocab_size = len(de_tokenizer.word_index) + 1\n",
    "            \n",
    "            model = create_translation_model(\n",
    "                activation,\n",
    "                en_vocab_size,\n",
    "                de_vocab_size,\n",
    "                d_model=64,\n",
    "                num_heads=2,\n",
    "                ff_dim=128,\n",
    "                num_layers=2\n",
    "            )\n",
    "\n",
    "            # Compile model\n",
    "            model.compile(\n",
    "                optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "            history = model.fit(\n",
    "                train_data, train_targets,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(val_data, val_targets),\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Evaluate model\n",
    "            val_loss, val_acc = model.evaluate(val_data, val_targets, verbose=0)\n",
    "            \n",
    "            # Generate predictions for a small subset for BLEU score\n",
    "            sample_size = min(100, len(val_data[0]))\n",
    "            predictions = model.predict([val_data[0][:sample_size], val_data[1][:sample_size]])\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = bleu_score(val_targets[:sample_size], predictions, de_tokenizer)\n",
    "\n",
    "            # Measure dead neurons\n",
    "            dead_neurons = measure_neuron_activity(model, train_data)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'name': name,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'bleu': bleu,\n",
    "                'training_time': training_time,\n",
    "                'dead_neurons': dead_neurons,\n",
    "                'history': history.history\n",
    "            })\n",
    "\n",
    "            print(f\"{name} - Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}, BLEU: {bleu:.4f}, Time: {training_time:.1f}s, Dead Neurons: {dead_neurons:.2%}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "# Visualize Results\n",
    "# ======================\n",
    "def visualize_results(results):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # 1. Validation Accuracy Plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for res in results:\n",
    "        if 'val_accuracy' in res['history']:\n",
    "            plt.plot(res['history']['val_accuracy'], label=res['name'])\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Test Performance (Accuracy & BLEU)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    names = [res['name'] for res in results]\n",
    "    acc = [res['val_acc'] for res in results]\n",
    "    bleu = [res['bleu'] * 100 for res in results]  # Scale BLEU score for better visualization\n",
    "\n",
    "    x = np.arange(len(names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, acc, width, label='Validation Accuracy')\n",
    "    plt.bar(x + width/2, bleu, width, label='BLEU Score * 100')\n",
    "\n",
    "    plt.title('Test Performance')\n",
    "    plt.xlabel('Activation Function')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, names, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # 3. Training Time\n",
    "    plt.subplot(2, 2, 3)\n",
    "    times = [res['training_time'] for res in results]\n",
    "    plt.bar(names, times)\n",
    "    plt.title('Training Time')\n",
    "    plt.xlabel('Activation Function')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Seconds')\n",
    "\n",
    "    # 4. Dead Neurons Percentage\n",
    "    plt.subplot(2, 2, 4)\n",
    "    dead = [res['dead_neurons'] for res in results]\n",
    "    plt.bar(names, dead)\n",
    "    plt.title('Dead Neurons Percentage')\n",
    "    plt.xlabel('Activation Function')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Percentage')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('wmt_activation_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "# ======================\n",
    "# Final Comparison\n",
    "# ======================\n",
    "def output_comparison_table(results):\n",
    "    # Sort by validation accuracy\n",
    "    sorted_results = sorted(results, key=lambda x: x['val_acc'], reverse=True)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'Activation': res['name'],\n",
    "            'Val Accuracy': f\"{res['val_acc']:.4f}\",\n",
    "            'Val Loss': f\"{res['val_loss']:.4f}\",\n",
    "            'BLEU Score': f\"{res['bleu']:.4f}\",\n",
    "            'Training Time (s)': f\"{res['training_time']:.1f}\",\n",
    "            'Dead Neurons (%)': f\"{res['dead_neurons']:.2%}\"\n",
    "        }\n",
    "        for res in sorted_results\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "# ======================\n",
    "# Main Execution\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Ensure NLTK BLEU calculation is available\n",
    "    import nltk\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    results = run_experiment()\n",
    "    visualize_results(results)\n",
    "    comparison_table = output_comparison_table(results)\n",
    "\n",
    "    print(\"\\nActivation Function Comparison (sorted by validation accuracy):\")\n",
    "    print(comparison_table)\n",
    "    comparison_table.to_csv('wmt_activation_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
