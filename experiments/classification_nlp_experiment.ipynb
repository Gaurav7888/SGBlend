{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models, constraints, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ======================\n",
    "# Custom Constraints\n",
    "# ======================\n",
    "class ClipConstraint(constraints.Constraint):\n",
    "    \"\"\"Clips weights between `min_val` and `max_val`.\"\"\"\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return tf.clip_by_value(w, self.min_val, self.max_val)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'min_val': self.min_val, 'max_val': self.max_val}\n",
    "\n",
    "# ======================\n",
    "# Custom Activation Layers\n",
    "# ======================\n",
    "\n",
    "\n",
    "class SGBlend(layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Learnable parameters\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\",\n",
    "            shape=(1,),\n",
    "            initializer=\"zeros\",  # Start with GELU (alpha=0)\n",
    "            constraint=tf.keras.constraints.MinMaxNorm(0.0, 1.0)\n",
    "        )\n",
    "\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",\n",
    "            shape=(1,),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            constraint=ClipConstraint(0.1, 10.0)\n",
    "        )\n",
    "\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\",\n",
    "            shape=(1,),\n",
    "            initializer=\"zeros\"\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        # GELU approximation: x * σ(1.702x)\n",
    "        gelu_term = x * tf.nn.sigmoid(1.702 * x)\n",
    "        # SSwish: x * σ(beta x) - gamma\n",
    "        sswish_term = x * tf.nn.sigmoid(self.beta * x) - self.gamma\n",
    "        # Blend using alpha (sigmoid to enforce [0,1])\n",
    "        alpha = tf.nn.sigmoid(self.alpha)  \n",
    "        return alpha * sswish_term + (1 - alpha) * gelu_term\n",
    "\n",
    "class SSwish(layers.Layer):\n",
    "    \"\"\"Symmetric Swish\"\"\"\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=(1,),\n",
    "            initializer='glorot_uniform',\n",
    "            constraint=ClipConstraint(0.1, 10)\n",
    "        )\n",
    "        self.gamma = self.add_weight(\n",
    "            name='gamma',\n",
    "            shape=(1,),\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * tf.nn.sigmoid(self.beta * x) - self.gamma\n",
    "\n",
    "class GELU(layers.Layer):\n",
    "    \"\"\"Gaussian Error Linear Unit\"\"\"\n",
    "    def call(self, x):\n",
    "        return tf.nn.gelu(x)\n",
    "\n",
    "class Mish(layers.Layer):\n",
    "    \"\"\"Mish Activation\"\"\"\n",
    "    def call(self, x):\n",
    "        return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# ======================\n",
    "# Load and Preprocess Data\n",
    "# ======================\n",
    "\n",
    "# We'll use IMDB dataset for sentiment analysis\n",
    "def load_data():\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
    "    \n",
    "    # Convert sequence of integers to words\n",
    "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    \n",
    "    # Decode sequences\n",
    "    def decode_review(text):\n",
    "        return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
    "    \n",
    "    # Decode the full dataset\n",
    "    x_train_decoded = [decode_review(review) for review in x_train]\n",
    "    x_test_decoded = [decode_review(review) for review in x_test]\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(x_train_decoded + x_test_decoded)\n",
    "    \n",
    "    # Convert to sequences\n",
    "    x_train_seq = tokenizer.texts_to_sequences(x_train_decoded)\n",
    "    x_test_seq = tokenizer.texts_to_sequences(x_test_decoded)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_len = 200\n",
    "    x_train_pad = pad_sequences(x_train_seq, maxlen=max_len, padding='post')\n",
    "    x_test_pad = pad_sequences(x_test_seq, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # No need to slice labels as we're using the full dataset\n",
    "    \n",
    "    return x_train_pad, y_train, x_test_pad, y_test, tokenizer.word_index\n",
    "\n",
    "# ======================\n",
    "# Custom Transformer Block\n",
    "# ======================\n",
    "\n",
    "def transformer_block(inputs, embed_dim, num_heads, ff_dim, activation, rate=0.1):\n",
    "    \"\"\"Transformer block with configurable activation function.\"\"\"\n",
    "    # Multi-head attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim\n",
    "    )(inputs, inputs)\n",
    "    attention_output = layers.Dropout(rate)(attention_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    if isinstance(activation, layers.Layer):\n",
    "        # For custom activation layers (which are layers themselves)\n",
    "        ffn_output = layers.Dense(ff_dim)(out1)\n",
    "        ffn_output = activation(ffn_output)\n",
    "        ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "    else:\n",
    "        # For built-in activation functions\n",
    "        ffn_output = layers.Dense(ff_dim, activation=activation)(out1)\n",
    "        ffn_output = layers.Dense(embed_dim)(ffn_output)\n",
    "\n",
    "    ffn_output = layers.Dropout(rate)(ffn_output)\n",
    "    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# ======================\n",
    "# Define Model Architecture\n",
    "# ======================\n",
    "\n",
    "def create_model(activation, vocab_size, embed_dim=32, num_heads=2, ff_dim=32):\n",
    "    \"\"\"\n",
    "    Create a small transformer model with the specified activation function.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(200,))\n",
    "    embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Add transformer blocks\n",
    "    x = transformer_block(x, embed_dim, num_heads, ff_dim, activation)\n",
    "    x = transformer_block(x, embed_dim, num_heads, ff_dim, activation)\n",
    "\n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Final classification layer\n",
    "    if isinstance(activation, layers.Layer):\n",
    "        x = layers.Dense(20)(x)\n",
    "        x = activation(x)\n",
    "    else:\n",
    "        x = layers.Dense(20, activation=activation)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ======================\n",
    "# Main Experiment\n",
    "# ======================\n",
    "\n",
    "def run_experiment():\n",
    "    # Load and preprocess data\n",
    "    x_train, y_train, x_test, y_test, word_index = load_data()\n",
    "\n",
    "    # Define activations to compare\n",
    "    activations = {\n",
    "        'SGBlend': SGBlend(),\n",
    "        'GELU': GELU(),\n",
    "        'Swish': tf.nn.swish,\n",
    "        'SSwish': SSwish(),\n",
    "        'ReLU': tf.nn.relu,\n",
    "        'Mish': Mish(),\n",
    "    }\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 15\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # For measuring neuron \"deadness\" in transformers\n",
    "    def measure_neuron_activity(model, x_data, layer_idx=-3):\n",
    "        # Create a submodel to extract intermediate activations\n",
    "        layer_outputs = [layer.output for layer in model.layers[:layer_idx]]\n",
    "        submodel = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "        # Get activations\n",
    "        activations = submodel.predict(x_data[:100])\n",
    "        penultimate_activations = activations[-1]  # Get the last layer\n",
    "\n",
    "        # Calculate percentage of dead neurons\n",
    "        if len(penultimate_activations.shape) > 2:\n",
    "            # For tensors with more than 2 dimensions, flatten the spatial dimensions\n",
    "            dead_neuron_pct = np.mean(np.max(penultimate_activations, axis=(0, 1)) < 1e-3)\n",
    "        else:\n",
    "            # For 2D tensors (batch_size, features)\n",
    "            dead_neuron_pct = np.mean(np.max(penultimate_activations, axis=0) < 1e-3)\n",
    "\n",
    "        return dead_neuron_pct\n",
    "\n",
    "    with tf.device('/GPU:0'):  # Use GPU if available\n",
    "        for name, activation in activations.items():\n",
    "            print(f\"\\nTraining with {name} activation...\")\n",
    "\n",
    "            # Create model\n",
    "            vocab_size = len(word_index) + 1\n",
    "            model = create_model(activation, vocab_size)\n",
    "\n",
    "            # Compile model\n",
    "            model.compile(\n",
    "                optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "            history = model.fit(\n",
    "                x_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.1,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Evaluate model\n",
    "            y_pred = (model.predict(x_test) > 0.5).astype(int).flatten()\n",
    "            test_acc = accuracy_score(y_test, y_pred)\n",
    "            test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Measure dead neurons\n",
    "            dead_neurons = measure_neuron_activity(model, x_train)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'name': name,\n",
    "                'test_acc': test_acc,\n",
    "                'test_f1': test_f1,\n",
    "                'training_time': training_time,\n",
    "                'dead_neurons': dead_neurons,\n",
    "                'history': history.history\n",
    "            })\n",
    "\n",
    "            print(f\"{name} - Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Time: {training_time:.1f}s, Dead Neurons: {dead_neurons:.2%}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ======================\n",
    "# Visualize Results\n",
    "# ======================\n",
    "\n",
    "def visualize_results(results):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # 1. Validation Accuracy Plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for res in results:\n",
    "        if 'val_accuracy' in res['history']:\n",
    "            plt.plot(res['history']['val_accuracy'], label=res['name'])\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # 2. Test Performance (Accuracy & F1)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    names = [res['name'] for res in results]\n",
    "    acc = [res['test_acc'] for res in results]\n",
    "    f1 = [res['test_f1'] for res in results]\n",
    "\n",
    "    x = np.arange(len(names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, acc, width, label='Accuracy')\n",
    "    plt.bar(x + width/2, f1, width, label='F1 Score')\n",
    "\n",
    "    plt.title('Test Performance')\n",
    "    plt.xlabel('Activation Function')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(x, names, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    # 3. Training Time\n",
    "    plt.subplot(2, 2, 3)\n",
    "    times = [res['training_time'] for res in results]\n",
    "    plt.bar(names, times)\n",
    "    plt.title('Training Time')\n",
    "    plt.xlabel('Activation Function')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Seconds')\n",
    "\n",
    "    # 4. Dead Neurons Percentage\n",
    "    plt.subplot(2, 2, 4)\n",
    "    dead = [res['dead_neurons'] for res in results]\n",
    "    plt.bar(names, dead)\n",
    "    plt.title('Dead Neurons Percentage')\n",
    "    plt.xlabel('Activation Function')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Percentage')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nlp_activation_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "# ======================\n",
    "# Final Comparison\n",
    "# ======================\n",
    "\n",
    "def output_comparison_table(results):\n",
    "    # Sort by test accuracy\n",
    "    sorted_results = sorted(results, key=lambda x: x['test_acc'], reverse=True)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'Activation': res['name'],\n",
    "            'Test Accuracy': f\"{res['test_acc']:.4f}\",\n",
    "            'F1 Score': f\"{res['test_f1']:.4f}\",\n",
    "            'Training Time (s)': f\"{res['training_time']:.1f}\",\n",
    "            'Dead Neurons (%)': f\"{res['dead_neurons']:.2%}\"\n",
    "        }\n",
    "        for res in sorted_results\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "# ======================\n",
    "# Main Execution\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    results = run_experiment()\n",
    "    visualize_results(results)\n",
    "    comparison_table = output_comparison_table(results)\n",
    "\n",
    "    print(\"\\nActivation Function Comparison (sorted by test accuracy):\")\n",
    "    print(comparison_table)\n",
    "    comparison_table.to_csv('nlp_activation_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
